{"cells":[{"metadata":{"_uuid":"4a55f2a03f50a60d860837480958f1d779645415"},"cell_type":"markdown","source":"### Import Data and Preprocessing Libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# Data Processing/Exploration Libraries\n#kaggle https://www.kaggle.com/c/costa-rican-household-poverty-prediction\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\npd.options.display.float_format = '{:.0f}'.format\n\n#Checking to see the expected submission format\nsample = pd.read_csv('../input/sample_submission.csv')\n\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Data Import & First Look\n#train = pd.read_csv('train.csv', index_col='Id')\ntrain = pd.read_csv('../input/train.csv')\nprint (\"Train dataset has {} rows(samples) with {} columns(features) each.\".format(*train.shape))\n\n#test = pd.read_csv('test.csv', index_col='Id')\ntest = pd.read_csv('../input/test.csv')\n\nprint (\"Test dataset has {} rows(samples) with {} columns(features) each.\".format(*test.shape))\n\n#Lets consolidate train and test for preprocessing purpose. We will then separate it out based on null value on Target column\ndata=train.append(test)\ndisplay(data.describe())\ndisplay(data.head())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07a727ca5b8e64038cc146a9920eacead8513351","trusted":true},"cell_type":"code","source":"def check_missing(data):\n    missing = data.isnull().sum().sort_values(ascending=False)\n    missing_percent= (missing[missing > 0]*100 / data.shape[0])\n    display(missing_percent)\ncheck_missing(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b468eae55ec3b8991a8be0b2e1cfa236721c033"},"cell_type":"markdown","source":"### Removing Columns with Null values\n\nOut of 142 features, we have 3 columns with 70 to 70% null values and 2 columns with .05% null values.\n\nWe can delete the observations with null meaneduc  and SQBmeaned   as it consists of less than .05% of the total population.\n\nrez_esc: Years behind in school:\n\nAlthough, this is a very important feature and have high priority, I decided to remove this features and 80+% of data is missing and we have some more features which gives an estimate of the education status in the houehold.\n\nv18q1, number of tablets household owns:\nNumber of tablets has 76% null values and can be ignore as we have V18q already which gives some insight on tablets owned by a person.\n\nv2a1: Monthly rent payment is a very important feature and can be estimated based on other house related features. We can go about two ways on estimating the missing values.\n- impute null values with median/mean after grouping by rent related features\n- train a separate model to predict a missing value\n- use some generative model, that can fill missing values by itself, one possibility is a Restricted Boltzmann Machine\n\nI will first try imputing  the missing value due to time constraint"},{"metadata":{"_uuid":"983eade1307a32003eb025b875ff0477b0a3aaae","trusted":true},"cell_type":"code","source":"'''For feature v2a1, mean is higher than median which implies the presence of outliers. In this case, median imputation will be more appropriate. We will also consider median by segments based on following rent related features:\nrooms,  number of all rooms in the house\nv14a, =1 has bathroom in the household\npisonotiene, =1 if no floor at the household\ncielorazo, =1 if the house has ceiling\nabastaguano, =1 if no water provision\nsanitario1, =1 no toilet in the dwelling\nenergcocinar1, =1 no main source of energy used for cooking (no kitchen)\n'''\n\ndata['v2a1'].describe()\n# For feature v2a1, mean is higher than median which implies the presence of outliers. \n# In this case, median imputation will be more appropriate after grouping by following rent related features.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2d3d41a251a06f31950073121d32f9cfb4eb7f6","trusted":true},"cell_type":"code","source":"data['v2a1']=data.groupby(['rooms','v14a','pisonotiene','cielorazo','abastaguano','sanitario1','energcocinar1'])['v2a1'].transform(lambda x: x.fillna(x.median()))\n\n#drop/impute columns and rows with null values count 2\ndata.drop(['rez_esc','v18q1'], axis=1,inplace=True)\n#drop columns which are square value of existing features. These can be ignored as they are redundant for out model. 9\ndata.drop(['SQBescolari', 'SQBage','SQBhogar_total','SQBedjefe','SQBhogar_nin','SQBovercrowding','SQBdependency','SQBmeaned','agesq'], axis=1,inplace=True)\n\ndata.dropna(axis=0,inplace=True,subset=['meaneduc','v2a1'])\ndata_dropped_rows=data[data['meaneduc'].isnull() | data['v2a1'].isnull()]\n\ncheck_missing(data)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6aba8bb9546a655b9214877113a3393349187907"},"cell_type":"markdown","source":"The dataset given to us is at indiviudal level while we have to predict target on a household level. Lets first consolidate the data on a household level. \nFor Household level information I will consolidate the data from Individuals and for houselevel information I will get the data from the head of household."},{"metadata":{"_uuid":"ef5cb40629ff855a5dad79c8c856abef4261abb3","trusted":true},"cell_type":"code","source":"has_household_head=data.groupby('idhogar')['parentesco1'].sum()\nhas_household_head[has_household_head != 1].count()\n#For household with no head, to avoid discrapancy and to be on the safe side,\n#we will choose the most represented value for each feature among the family members.\n#data.columns\ncorr=data.corr().abs()\n\nindices = np.where(corr > 0.90)\nindices = [(corr.index[x], corr.columns[y]) for x, y in zip(*indices)\n                                        if x != y and x < y]\nindices","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbc57dfd735dd93692a97c4aab9cbadfd838f549","trusted":true},"cell_type":"code","source":"'''Duplicate Columns:\n\nhhsize, household size\nhogar_total, # of total individuals in the household\ntamhog, size of the household\nr4t3, Total persons in the household\ntamviv, number of persons living in the household\n\nabastaguadentro, =1 if water provision inside the dwelling\nabastaguafuera, =1 if water provision outside the dwelling\n\npublic, \"=1 electricity from CNFL,  ICE,  ESPH/JASEC\"\ncoopele, =1 electricity from cooperative\n\nmale, =1 if male\nfemale, =1 if female\n\nsanitario2, =1 toilet connected to sewer or cesspool\nsanitario3, =1 toilet connected to  septic tank\n\nenergcocinar2, =1 main source of energy used for cooking electricity\nenergcocinar3, =1 main source of energy used for cooking gas\n\narea1, =1 zona urbana\narea2, =2 zona rural\n\nColumns that can be dropped are:\nhhsize, household size\ntamhog, size of the household\nr4t3, Total persons in the household\ntamviv, number of persons living in the household\nabastaguadentro, =1 if water provision inside the dwelling\nmale, =1 if male\narea1, =1 zona urbana\n\n'escolari', 'age' can also be dropped since it is not effective on a household level and we have \nother source to find the number of people under certain age\n\n'dependency', 'edjefe', 'edjefa', These have mixed data and to avoidmisinterpreting, we will remove them as well.\n'''\n\ndata.drop(['hhsize', 'tamhog','r4t3','tamviv','abastaguadentro','area1','escolari', 'age','dependency', 'edjefe', 'edjefa'], axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69ea7346fee30d2de6f38f175271c1d31d0f98f5","trusted":true},"cell_type":"code","source":"display(data.shape)\ndisplay(data.dtypes.unique())\nbool_features=[]\ng = data.columns.to_series().groupby(data.dtypes).groups\n{k.name: v for k, v in g.items()}\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43caae379dbb4d1606525e604b3028d851e6e854"},"cell_type":"markdown","source":"Now we will look through each remaining columns to see which are household related and which are individual"},{"metadata":{"_uuid":"3cae941300350cd10e716766890ae46912451a98","trusted":true},"cell_type":"code","source":"id_ = ['Id', 'idhogar', 'Target']\n\nind_features = ['v18q', 'dis','male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone']\n\n\n\nhh_features = ['idhogar', 'Target','hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n            'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area2', 'rooms', 'r4h1', 'r4h2', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'hogar_nin','hogar_adul','hogar_mayor','hogar_total','bedrooms','qmobilephone',\n                'overcrowding']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca959756808803ca5db04e5f19da1f286cc80822"},"cell_type":"code","source":"train['dependency'].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38b49ed221147c8ae5a143563645569daccf058b","trusted":true},"cell_type":"code","source":"ind_features_aggregate = data.groupby('idhogar')[ind_features].sum()\nind_features_aggregate.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27678c8cd56e9fe7e49e0949f0bf694b80727ab5","trusted":true},"cell_type":"code","source":"hh_features_head=data[data['parentesco1'] == 1]\nhh_features_nohead=data[(data['parentesco1']) != 1]\nhh_features_nohead_max = hh_features_nohead.groupby('idhogar')[hh_features].max()\n#hh_features_head.append(hh_features_nohead_max)\n\nhh_feature_list=hh_features_head[hh_features]\nhh_feature_list.append(hh_features_nohead_max)\nhh_feature_list.set_index('idhogar', inplace=True)\nhh_feature_list.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"593f97951f2e9ee0321e11826b51ceb78e453646","trusted":true},"cell_type":"code","source":"final=  hh_feature_list.join(ind_features_aggregate, how='inner')\nfinal.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a14c2ccadb76a4eb1ade790b7133e0ac6847ef44"},"cell_type":"code","source":"# Labels for training\ntrain_labels = np.array(list(final[final['Target'].notnull()]['Target'].astype(np.uint8)))\n\n# Extract the training data\ntrain_set = final[final['Target'].notnull()].drop(columns = [ 'Target'])\ntest_set = final[final['Target'].isnull()].drop(columns = [  'Target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0790f7c1b79f579d841d02bfec1f451e1f3147b9"},"cell_type":"code","source":"# Submission base which is used for making submissions to the competition\nsubmission_base = test['idhogar'].copy()\nsubmission_base.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb29f8f4deba02b3212f5a1c797c40fadf56dd74"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train_set, train_labels, test_size=0.25, random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d15c7792889a109e3a7128f4cc6db7ed540e6e0d","trusted":true},"cell_type":"code","source":"#Baseline Model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d15c7792889a109e3a7128f4cc6db7ed540e6e0d","trusted":true},"cell_type":"code","source":"predictions = logreg.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53baba17d13b788e9a7683fc4d936ac18648250a"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\ncm = metrics.confusion_matrix(y_test, predictions)\nprint(cm)\n\n# Use score method to get accuracy of model\nf1_score = f1_score(y_test,predictions, average='weighted')\nprint(f1_score)\n\nplt.figure(figsize=(9,9))\nsns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\nplt.ylabel('Actual label');\nplt.xlabel('Predicted label');\nall_sample_title = all_sample_title = 'F1 Score: {0}'.format(f1_score)\nplt.title(all_sample_title, size = 15);\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb5602c7d8f460b65bc4b5dbb1b8acdcb3ac4851"},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import make_scorer\nfrom sklearn.grid_search import GridSearchCV \nfrom sklearn.metrics import fbeta_score,accuracy_score,f1_score\n \nclf= XGBClassifier()\n\nparameters = {'learning_rate':[.05,.1,.2,.3], 'max_depth':[6,7,8,9], 'min_child_weight':[1,2], \n              'gamma':[0], 'subsample':[1], 'colsample_bytree':[.3,.4,.5], 'n_estimators':[100,150]\n             , 'reg_lambda':[1]}\n\n# TODO: Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(fbeta_score, beta=.5,average='weighted')\n\n# TODO: Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf,parameters,scorer)\n\n\n# TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(x_train,y_train)\n\n# Get the estimatorc\nbest_clf = grid_fit.best_estimator_\nbest_params=grid_fit.best_params_\n\n\n# Make predictions using the unoptimized and the optimized model \npredictions = (clf.fit(x_train, y_train)).predict(x_test)\nbest_predictions = best_clf.predict(x_test)\n\n# Report the before-and-afterscores\nprint (\"Unoptimized model\\n------\")\nprint (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\nprint (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5,average='weighted')))\nprint (\"\\nOptimized Model\\n------\")\nprint (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint (\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5,average='weighted')))\nprint (\"Best estimator parameters\")\nprint (best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b0c7f35974ea8bd68e56f06297bf586fd30a8d9"},"cell_type":"code","source":"cm = metrics.confusion_matrix(y_test, predictions)\nprint(cm)\n\n# Use score method to get accuracy of model\nf1score = f1_score(y_test,best_predictions, average='weighted')\n#score = metrics.accuracy_score(y_test, predictions)\n#print(f1_score)\n\nplt.figure(figsize=(9,9))\nsns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\nplt.ylabel('Actual label');\nplt.xlabel('Predicted label');\nall_sample_title = all_sample_title = 'F1 Score: {0}'.format(f1score)\nplt.title(all_sample_title, size = 15);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa57b841004b660baec1172f327125a514aa8c33"},"cell_type":"code","source":"test_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ba7c76db64fbbf5693124fcc577d18ba5e2d616"},"cell_type":"code","source":"## submission\nbest_predictions = best_clf.predict(test_set)\nsubs = pd.DataFrame(index=test_set.index)\nsubs['Target'] = best_predictions.astype(np.int64)\n#display(test_id.head())\ntest_id = pd.DataFrame(index=test['idhogar'])\ntest_id['Id']=test.index\n\nfinal=test_id.join(subs,  how='left')\nfinal['Id'] = 'ID_'+final.index\nfinal.reset_index(drop=True, inplace=True)\nfinal.to_csv('sample_submission.csv', index=False,float_format='%.0f')\ndisplay(final.shape)\nfinal.head()\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}