{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection_DS_takehome_challenges: Marketing Email Campaign\n",
    "\n",
    "### Kanja Saha\n",
    "### 08/12/2018\n",
    "\n",
    "The marketing team of an e-commerce site has launched an email campaign. This site has email addresses from all the users who created an account in the past. \n",
    "\n",
    "They have chosen a random sample of users and emailed them. The email let the user know about a new feature implemented on the site. From the marketing team perspective, a success is if the user clicks on the link inside of the email. This link takes the user to the company site. \n",
    "\n",
    "You are in charge of ﬁguring out how the email campaign performed and were asked the following questions:\n",
    "\n",
    "- What percentage of users opened the email and what percentage clicked on the link within the email? The VP of marketing thinks that it is stupid to send emails to a random subset and in a random way. Based on all the information you have about the emails that were sent, can you build a model to optimize in future email campaigns to maximize the probability of users clicking on the link inside the email? By how much do you think your model would improve click through rate ( deﬁned as # of users who click on the link / total users who received the email). How would you test that? Did you ﬁnd any interesting pattern on how the email campaign performed for diﬀerent segments of users? Explain.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Import Libraries & Data Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "from os import listdir\n",
    "from matplotlib import pyplot as plt\n",
    "import glob, re\n",
    "from datetime import datetime,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c1d4e4c3259c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'*.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "df1=pd.read_csv('....../email/email_opened_table.csv')\n",
    "df2=pd.read_csv('....../email/link_clicked_table.csv')\n",
    "df3=pd.read_csv('....../email/email_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Find the customer who bought the most items overall in her lifetime \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= (purchase_history.drop('id', axis=1)\n",
    "             .join\n",
    "             (\n",
    "             purchase_history.id\n",
    "             .str\n",
    "             .split(\",\")\n",
    "             .apply(pd.Series, 1)\n",
    "             .stack()\n",
    "             .reset_index(drop=True, level=1)\n",
    "             .rename('Item_id') \n",
    "             .astype(\"Int64\")\n",
    "             ))\n",
    "\n",
    "user_with_maxitem=df.groupby(\"user_id\").size().sort_values(ascending=False).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the customer who bought the most items overall in her lifetime is: 269335\n"
     ]
    }
   ],
   "source": [
    "users_with_maxitem = df.groupby(\"user_id\").size().rank(method='dense').astype(int).idxmax()\n",
    "print(\"the customer who bought the most items overall in her lifetime is: {}\".format(users_with_maxitem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "For each item,find the customer who bought that product the most \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"for each item, the customer who bought that product the most\")\n",
    "\n",
    "df1=df.groupby(['Item_id','user_id']).apply(lambda x: pd.Series(dict(Item_Total=x.Item_id.count())))\n",
    "df1.assign(rnk=df1.groupby(['Item_id','user_id'])['Item_Total'].rank(method='first', ascending=False)).query('rnk < 1').sort_values(['Item_id','Item_Total'])\n",
    "#p=df.groupby(['Item_id','user_id']).size().rename(\"Item_Total\").reset_index().sort_values(by=['Item_id','Item_Total'], ascending=False).groupby(\"Item_id\").head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Cluster items based on user co-purchase history. That is, create clusters of products that have the highest probability of being bought together. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=purchase_history[\"id\"].str.get_dummies(',').replace('',0)\n",
    "df2.rename(inplace=True,columns=dict(zip(df2.columns, item_to_id[\"Item_name\"])))\n",
    "n_data=df2.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow method & silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "cluster_range = range( 2, 20 )\n",
    "cluster_errors = []\n",
    "\n",
    "for num_clusters in cluster_range:\n",
    "    clusters = KMeans(num_clusters)\n",
    "    clusters.fit(n_data)\n",
    "    preds = clusters.predict(n_data)\n",
    "    cluster_errors.append( clusters.inertia_ )\n",
    "    score = silhouette_score(n_data, preds, metric='euclidean')\n",
    "    print (\"For K-means n_clusters = {}. The average silhouette_score is : {}\".format(num_clusters, score))\n",
    "\n",
    "clusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is no clear bent in the elbow, if you look at the silhoutte score around 12 to 14 cluster, the value seems to not change much at that range. So, I think 12 to 14 cluster may be considered optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find cluster centers and size using Kmeans\n",
    "# Loop through clusters\n",
    "if 'cluster' in n_data.columns:\n",
    "    n_data.drop(['cluster'], axis = 1, inplace = True)\n",
    "if 'item' in n_data.columns:\n",
    "    n_data.drop(['item'], axis = 1, inplace = True)\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "range_n_clusters = range(12,13)\n",
    "for n_clusters in range_n_clusters:\n",
    "    #Apply your clustering algorithm of choice to the reduced data \n",
    "    clusterer = KMeans(n_clusters=n_clusters).fit(n_data)\n",
    "\n",
    "    #Predict the cluster for each data point\n",
    "    preds = clusterer.predict(n_data)\n",
    "    \n",
    "    #Predict the cluster for each transformed sample data point\n",
    "    labels_kmeans=clusterer.labels_\n",
    "    n_data['cluster']=labels_kmeans\n",
    "n_data['item']=n_data.index\n",
    "n_data.sort_values(by =['cluster', 'item'], inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Kmeans result with 12 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 8), dpi=80)\n",
    "plt.xticks(rotation=90)\n",
    "ax=sns.barplot(n_data['item'],n_data['cluster'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "if 'cluster' in n_data.columns:\n",
    "    n_data.drop(['cluster'], axis = 1, inplace = True)\n",
    "if 'item' in n_data.columns:\n",
    "    n_data.drop(['item'], axis = 1, inplace = True)\n",
    "\n",
    "Hclustering = AgglomerativeClustering(n_clusters=12, affinity='euclidean', linkage='ward')\n",
    "Hclustering.fit_predict(n_data)\n",
    "labels_HC = Hclustering.labels_\n",
    "n_data['cluster']=Hclustering.labels_\n",
    "n_data['item']=n_data.index\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "    display(distance)\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "    display(no_of_observations)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    #dendrogram(linkage_matrix, p=3, truncate_mode='level', **kwargs)\n",
    "    dendrogram(linkage_matrix, p=30, truncate_mode='level', **kwargs)\n",
    "\n",
    "plt.figure(figsize=(20, 8), dpi=80)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(Hclustering, labels=Hclustering.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 8), dpi=80)\n",
    "plt.xticks(rotation=90)\n",
    "ax=sns.barplot(n_data['item'],n_data['cluster'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "From the two above clustering results, the grocery items can be clustered into 12 groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=pd.DataFrame(n_data.groupby(['cluster'])['item'].apply(list))\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "p.to_html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
