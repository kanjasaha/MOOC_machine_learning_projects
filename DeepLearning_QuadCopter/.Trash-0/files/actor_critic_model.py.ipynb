{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "class Actor:\n",
    "    \"\"\"\n",
    "    This class defines the actor (policy) model for DDPG.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task, lr_actor):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build model.\n",
    "        \"\"\"\n",
    "        # type of task (takeoff, hover or landing etc.)\n",
    "        self.task = task\n",
    "        # size of state/action space\n",
    "        self.state_size = task.state_size # size of state space\n",
    "        self.action_size = task.action_size # size of action space\n",
    "        # range of the action space\n",
    "        self.action_low = self.task.action_low # min in the action space\n",
    "        self.action_high = self.task.action_high # max in the action space\n",
    "        self.action_range = self.action_high - self.action_low # range of action space\n",
    "\n",
    "        self.lr_actor = lr_actor # learning rate for actor model\n",
    "        # build a model\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Define a neural network for an actor (policy) model,\n",
    "        i.e. the input is states and actions are returned.\n",
    "        \"\"\"\n",
    "        # input layer (input = states)\n",
    "        states = layers.Input(shape=(self.state_size,), name='states')\n",
    "\n",
    "        # hidden layers\n",
    "        net = layers.Dense(units=32, activation='relu')(states)\n",
    "        net = layers.BatchNormalization()(net)\n",
    "        net = layers.Dropout(0.5)(net)\n",
    "        net = layers.Dense(units=64, activation='relu')(net)\n",
    "        net = layers.BatchNormalization()(net)\n",
    "        net = layers.Dropout(0.5)(net)\n",
    "        net = layers.Dense(units=32, activation='relu')(net)\n",
    "\n",
    "        # output layer with sigmoid activation function (to be normalized below)\n",
    "        raw_actions = layers.Dense(units=self.action_size,\n",
    "                                        activation='sigmoid',name='raw_actions')(net)\n",
    "\n",
    "        # Rescaling of the output (s.t. the output take the value in the action space)\n",
    "        actions = layers.Lambda(lambda x: (x * self.action_range) + self.action_low,\n",
    "                                        name='actions')(raw_actions)\n",
    "\n",
    "        # Create Keras model\n",
    "        self.model = models.Model(inputs=states, outputs=actions)\n",
    "\n",
    "        # Define loss function using action-value (Q-value) gradients\n",
    "        # Note: action_gradients is computed in the class Critic\n",
    "        action_gradients = layers.Input(shape=(self.action_size,))\n",
    "        loss = K.mean(-action_gradients * actions)\n",
    "\n",
    "        # Define optimizer and training function\n",
    "        optimizer = optimizers.Adam(lr=self.lr_actor)\n",
    "        updates_op = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n",
    "        self.train_fn = K.function(\n",
    "                inputs=[self.model.input, action_gradients, K.learning_phase()],\n",
    "                outputs=[],\n",
    "                updates=updates_op)\n",
    "\n",
    "class Critic:\n",
    "    \"\"\"\n",
    "    This class defines the critic (value) model for DDPG.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task, lr_critic):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build model.\n",
    "        \"\"\"\n",
    "        # type of task (takeoff, hover or landing etc.)\n",
    "        self.task = task\n",
    "        # size of state/action space\n",
    "        self.state_size = task.state_size # size of state space\n",
    "        self.action_size = task.action_size # size of action space\n",
    "\n",
    "        self.lr_critic = lr_critic # learning rate for crtic model\n",
    "\n",
    "        # build a model\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Define a neural network for a critic (value) model,\n",
    "        i.e. the input is states and actions, and Q-value is returned\n",
    "        (its gradient is also computed).\n",
    "        \"\"\"\n",
    "        # Input layer (states and actions)\n",
    "        states = layers.Input(shape=(self.state_size,), name='states')\n",
    "        actions = layers.Input(shape=(self.action_size,), name='actions')\n",
    "\n",
    "        # Hidden layers for state pathway\n",
    "        net_states = layers.Dense(units=32, activation='relu')(states)\n",
    "        net_states = layers.BatchNormalization()(net_states)\n",
    "        net_states = layers.Dropout(0.5)(net_states)\n",
    "        net_states = layers.Dense(units=64, activation='relu')(net_states)\n",
    "        net_states = layers.BatchNormalization()(net_states)\n",
    "        net_states = layers.Dropout(0.5)(net_states)\n",
    "        net_states = layers.Dense(units=32, activation='relu')(net_states)\n",
    "\n",
    "        # Hidden layers for action pathway\n",
    "        net_actions = layers.Dense(units=32, activation='relu')(actions)\n",
    "        net_actions= layers.BatchNormalization()(net_actions)\n",
    "        net_actions = layers.Dropout(0.5)(net_actions)\n",
    "        net_actions = layers.Dense(units=64, activation='relu')(net_actions)\n",
    "        net_actions= layers.BatchNormalization()(net_actions)\n",
    "        net_actions = layers.Dropout(0.5)(net_actions)\n",
    "        net_actions = layers.Dense(units=32, activation='relu')(net_actions)\n",
    "\n",
    "        # Combine state and action pathways\n",
    "        net = layers.Add()([net_states, net_actions])\n",
    "\n",
    "        # # Hidden layers after combining the state and action pathways\n",
    "        net = layers.Dropout(0.5)(net)\n",
    "        net = layers.Dense(units=32, activation='relu')(net)\n",
    "\n",
    "        # Output layer (output = Q values)\n",
    "        Q_values = layers.Dense(units=1, name='q_values')(net)\n",
    "\n",
    "        # Create Keras model\n",
    "        self.model = models.Model(inputs=[states, actions], outputs=Q_values)\n",
    "\n",
    "        # Define optimizer and compile model for training with built-in loss function\n",
    "        optimizer = optimizers.Adam(lr=self.lr_critic)\n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Compute action gradients (derivative of Q values w.r.t. to actions)\n",
    "        # Note: this action_gradients is used in the actor model\n",
    "        action_gradients = K.gradients(Q_values, actions)\n",
    "\n",
    "        # Define an additional function to fetch action gradients (to be used by actor model)\n",
    "        self.get_action_gradients = K.function(\n",
    "                        inputs=[*self.model.input, K.learning_phase()],\n",
    "                        outputs=action_gradients)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
