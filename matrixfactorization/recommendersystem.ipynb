{
 "cells": [
  {
   "attachments": {
    "root-mean-square-error.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project\n",
    "Kanja Saha  \n",
    "January 15, 2018\n",
    "\n",
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "_Build a Product Recommendation Engine with item to item Collaborative Filtering technique using Matrix Factorization_\n",
    "\n",
    "Some say customer is God and others say customer is King. Some say, \"Listen to your customer!\" while others say \"Know thy customer!\". The latter is my personal marketing philosophy!\n",
    "\n",
    "A company exists because of its customers; to be precise, its loyal customers. \n",
    "A loyal customer is a satisfied customer whose trust you have earned. And the word \"trust\" carries a lot of weight and is only earned through consistent positive value of product, service and experience and show that you \"know your customer\" . To reflect that you indeed know your customer, it is imperative that you recommend only the products that you believe will benefit the customer. \n",
    "\n",
    "Hence a product recommendation engine is crucial for every company’s success. In fact it is necessary for success of every department of the company. A recommender system is already in place for large market place such as Amazon, Google and other organizations, small or large, are inspired to build a near accurate recommender system. As I got introduced to Nearest Neighor Algorithm in Unsupervised Learning in Udacity’s Machine Learning Nanodegree Program, I realized that clustering is at the base of Recommender System. I then researched further to find Collaborative Filtering technique implementing Matrix Factorization to be one of the popular methods to build an efficient recommender engine. \n",
    "\n",
    "In the more general sense, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc. In terms of recommendation system, it is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating) and finding similarities between items based on user feedback. In the mathematical discipline of linear algebra, a matrix decomposition or matrix factorization is a dimensionality reduction technique that factorizes a matrix into a product of matrices, usually two. \n",
    "\n",
    "In building Recconmendation System using item to item Collaborative Filtering technique, we generate two matrices through convergence: an user matrix with latent features of the users  and an item matrix with a few latent features of the Items. We then take the item similarity matrix and assign rating to users based on previous ratings of the user on similar items.\n",
    "\n",
    "I will use Amazon Product Ratings Only  Dataset for this project.\n",
    "These datasets include no metadata or reviews, but only (user,item,rating,timestamp) tuples. I will use reviewerID(user), asin(item) and overall(rating) columns for my project.\n",
    "\n",
    "Following are the details on the dataset.\n",
    "\n",
    "- reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "- asin - ID of the product, e.g. 0000013714\n",
    "- overall: rating of the product,\n",
    "- reviewTime - time of the review (raw)\n",
    "\n",
    "Sample Ratings Only Data:\n",
    "{\n",
    "  \"reviewerID\": \"A2SUAM1J3GNN3B\",\n",
    "  \"asin\": \"0000013714\",\n",
    "  \"overall\": 5.0,\n",
    "  \"reviewTime\": \"09 13, 2009\"\n",
    "}\n",
    "\n",
    "Source for the data: http://jmcauley.ucsd.edu/data/amazon/links.html\n",
    "\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://www.quora.com/What-is-a-laymans-explanation-of-matrix-factorization-in-collaborative-filtering\n",
    "http://vcp.med.harvard.edu/papers/matrices-3.pdf\n",
    "http://mahout.apache.org/users/recommender/matrix-factorization.html\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "With the advent of internet commerce, it has become convenient for the users to find the items of their interest without stepping out of their house. However with the consistent  increase of number of online retailers it is also very difficult for the user to find the items that really meets their need the best. Users feel lost in the sea of products available to them and often fear of making a wrong purchasing decision. And once they make a purchase that is not ideal, customers shy away from making further online purchase investment or recommending others. This is a no win situation for the user as well as the company selling the product\n",
    "\n",
    "Recommendation system is considered a semi supervised learning or a combination of supervised(ranking a recommended item) and unsupervised learning(forming clusters of similar groups of customers/items). But overall it is an information retrieval system, which is another large area of machine learning.\n",
    "\n",
    "To make a buyer feel confident about making frequent purchase decision, there is a need for a system which learns the user preferences, spending pattern and generate recommendations based on his interest and past buying habit, The Recommender System. I believe, one of the best ways to build the recommendation engine is by using Collaborative Filtering technique. Collaborative filtering is the technique of recommending items to users based on past interactions between users and items. \n",
    "\n",
    "So, my goal is to build a recommendation system with item to item Collaborative Filtering Technique. I will implement the Matrix Factorization algorithms in sklearn Library: Non-Negative Matrix Factorization.\n",
    "\n",
    "\n",
    "### Metrics\n",
    "\n",
    "I will use Root Mean Square Error (RMSE) as evaluation metrics.\n",
    "\n",
    "Root mean squared error (RMSE): RMSE is a quadratic scoring rule that also measures the average magnitude of the error. It’s the square root of the average of squared differences between prediction and actual observation.\n",
    "\n",
    "![root-mean-square-error.png](attachment:root-mean-square-error.png)\n",
    "\n",
    "where n is the total number of observations, and Z(fi) and Z(oi) are forcasted and observed values of an observation i.\n",
    "\n",
    "Reference:\n",
    "\n",
    "http://www.statisticshowto.com/rmse/\n",
    "https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis\n",
    "\n",
    "### Data Exploration & Visualization\n",
    "\n",
    "Amazon ratings source dataset has around 82.5 million rows and 4 columns. The first three columns, reviewerID, asin and overall are required for this project. The dataset has 21,176,522 unique reviewers and 9,874,211 items. <br>\n",
    "\n",
    "Due to limitation of my personal computer performance as well as more accuracy. The final dataset of 10 million rows and 3 columns consist of a set of 576,489 reviewers who have rated atleast 20 items as well 112,980 items that are rated by atleast 100 reviewers.<br>\n",
    "This filtered dataset does not have any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Libraries and Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns; \n",
    "import matplotlib.pyplot as plt;\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import NMF\n",
    "from IPython.display import display\n",
    "import itertools\n",
    "\n",
    "\n",
    "#raw_data= pd.read_csv(\"item_dedup.csv\", skiprows=2000000,nrows=1000000, header=None)\n",
    "raw_data_step1= pd.read_csv(\"item_dedup.csv\", nrows=7000000, header=None)\n",
    "raw_data_step1.columns = ['reviewerID', 'asin','overall','reviewTime']\n",
    "raw_data_step1.drop(['reviewTime'], axis = 1, inplace = True)\n",
    "\n",
    "raw_data=raw_data_step1.assign(rnk=raw_data_step1.groupby(['overall'])['asin']\n",
    "                    .rank(method='min', ascending=False)) .query('rnk < 500000') \n",
    "raw_data.drop(['rnk'], axis = 1, inplace = True)\n",
    "\n",
    "display(raw_data.shape)\n",
    "#raw_data.isnull().any()\n",
    "raw_data.head()\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "plt.figure('overall')\n",
    "plt.ylabel('Count of Ratings')\n",
    "plt.xlabel('Ratings 1 to 5')\n",
    "sns.distplot(raw_data['overall'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Subset of the data\n",
    "\n",
    "item_rating_count=raw_data.groupby('asin').aggregate({'reviewerID': np.count_nonzero})\n",
    "item_rating_count_gte_200 = item_rating_count[item_rating_count.reviewerID > 200]\n",
    "\n",
    "asin_rating_count_gte_200=item_rating_count_gte_200.index\n",
    "\n",
    "shortlisted_data = raw_data[raw_data.asin.isin(asin_rating_count_gte_200)]\n",
    "\n",
    "final_data=shortlisted_data.assign(rnk=shortlisted_data.groupby(['asin'])['reviewerID']\n",
    "                    .rank(method='min', ascending=False)) .query('rnk < 25') \n",
    "final_data.drop(['rnk'], axis = 1, inplace = True)\n",
    "\n",
    "display(final_data.isnull().any())\n",
    "display(final_data.shape)\n",
    "display(final_data.nunique())\n",
    "final_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "\n",
    "The count of items for each ratings in the dataset is obtained and the graph is plotted as shown below in Figure 1. From the graph it is obvious that users have mostly rated items as 4 or 5, which is a good news because this implies there is high potential for product recommendation of similar items.\n",
    "About 50% of all ratings are 5, 30% of all ratings are 4, 5% are 3 and the rest are 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating and its count\n",
    "sns.set(color_codes=True)\n",
    "plt.figure('overall')\n",
    "plt.ylabel('Count of Ratings')\n",
    "plt.xlabel('Ratings 1 to 5')\n",
    "sns.distplot(final_data['overall'])\n",
    "plt.show()\n",
    "\n",
    "display(final_data['overall'].describe([.1,.15,.2,.3,.4,.5,.6,.7,.8,.9]).apply(lambda x: '%.0f' % x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n",
    "\n",
    "I will use Non-Negative Matrix Factorization (NMF) model which is implemented in sklearn library in Python.<br> \n",
    "\n",
    "Non-Negative Matrix Factorization (NMF) is a recent technique for linear dimensionality reduction and data analysis that yields a parts based, sparse non-negative representation for non-negative input data. Essentialy, NMF is an unsupervised learning algorithm coming from linear algebra that not only reduces data dimensionality, but also performs clustering simultaneously.<br>\n",
    "\n",
    "70% of the items in this dataset has receieved rating from .05% of total reviewers, this can be considered a sparse matrix. NMF tend to perform better than SVD and other matrix factorization alogorithms for sparse matrix. <br>\n",
    "\n",
    "NMF finds two non-negative matrices (W, H) whose product approximates the original non- negative matrix X.<br>\n",
    "\n",
    "NMF has quite a few parameters but the most deciding factor is n_components, an integer or none.\n",
    "n_components is the total number of latest features of the factored Matrices. The optimum value for n_components  can be generated by implementing NMF model in iterative fashion with different n_components and generate the RMSE value for each value of n_components in NMF. The n_components(latent features) with least RMSE score will be used to find the RMSE score in test dataset.\n",
    "\n",
    "The parameters and attribute of NMF in sklearn are noted as below:\n",
    "class sklearn.decomposition.NMF(n_components=None, init=None, solver='cd', beta_loss='frobenius', tol=0.0001, max_iter=200, random_state=None, alpha=0.0, l1_ratio=0.0, verbose=0, shuffle=False)\n",
    "\n",
    "The details of this class and its default parameters can be found in sklearn documentation.<br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(item_rating_count_gte_200['reviewerID'].describe([.1,.15,.2,.3,.4,.5,.6,.7,.8,.9]).apply(lambda x: '%.0f' % x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "I will use nearest neighors algorithm in sklearn library to generate the benchmark numbers.\n",
    "\n",
    "The benchmark score for RMSE is 4 with k neighors and are generated by implementing nearest neighors algorithm in the Methodology/Implementation Section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology\n",
    "_(approx. 3-5 pages)_\n",
    "\n",
    "### Data Preprocessing\n",
    "The final dataset is pivoted to create a sparse dataframe with \"reviewerID\" as Index, \"asin\" as Column and \"overall\" as value. <br>This final dataframe has n rows(reviewers) and n columns(items).\n",
    "\n",
    "We then split this dataframe into train and test dataset(20% for testing). 10 fold validation will be performed on training dataset to find the optimum k value for k neighors algorithm and optimum p latent features for Non Negative Matrix Factorization based on their RMSE and MAE scores.\n",
    "\n",
    "These k and p values will then be used on test dataset for kneighors and NMF algorithm. The RMSE and MAE score from kneighors is will be considered as baseline and will be used as benchmark to compare the performance of NMF algorithm based on the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Data\n",
    "#final_data=raw_data\n",
    "pivot_data=pd.pivot_table(final_data, values = 'overall', index='reviewerID', columns ='asin')\n",
    "pivot_data.fillna(0, inplace=True)\n",
    "display(pivot_data.shape)\n",
    "\n",
    "train_data, test_data = train_test_split(pivot_data, test_size=0.2)\n",
    "\n",
    "display(train_data.head())\n",
    "test_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train_data.columns\n",
    "bt = train_data.apply(lambda x: x > 1)\n",
    "bt.apply(lambda x: list(cols[x.values]), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "* Programming Language: Python 3.6\n",
    "* Libraries : Pandas,Numpy, Scikit-learn\n",
    "* Goal: Implement NMF in sklearn to build a recommender system using item to item collaborative filtering technique\n",
    "* Workflow :\n",
    "    * Establish the baselines with K-nearest neighbors for comparison.\n",
    "\n",
    "\n",
    "In this section, the process for which metrics, algorithms, and techniques that you implemented for the given data will need to be clearly documented. It should be abundantly clear how the implementation was carried out, and discussion should be made regarding any complications that occurred during this process. Questions to ask yourself when writing this section:\n",
    "- _Is it made clear how the algorithms and techniques were implemented with the given datasets or input data?_\n",
    "- _Were there any complications with the original metrics or techniques that required changing prior to acquiring a solution?_\n",
    "- _Was there any part of the coding process (e.g., writing complicated functions) that should be documented?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def join_groundtruth_predicted_value(v_item_similarity,v_raw_data,v_data):\n",
    "    \n",
    "    item_prediction = v_data.dot(v_item_similarity) /((v_item_similarity).sum(axis=1))\n",
    "        \n",
    "    item_prediction['reviewerID']=item_prediction.index\n",
    "    prediction_melt=pd.melt(item_prediction, id_vars=['reviewerID'], var_name='asin', value_name='overall_pred')\n",
    "     \n",
    "    final=pd.merge(v_raw_data, prediction_melt, how='inner', on=['reviewerID','asin'])\n",
    "    return final\n",
    "\n",
    "\n",
    "\n",
    "def get_item_similarity_knn(v_data,v_i):\n",
    "    v_data_t=v_data.transpose()\n",
    "    model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute',n_neighbors=v_i)\n",
    "    model_knn.fit(v_data_t)\n",
    "    itemtoitem_1=model_knn.kneighbors_graph(v_data_t)\n",
    "    itemtoitem=itemtoitem_1.toarray()\n",
    "    item_similarity=pd.DataFrame(itemtoitem, columns=v_data.columns,index=v_data.columns)\n",
    "   \n",
    "    return item_similarity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors_approx=np.floor(sqrt(train_data.shape[1])).astype(np.int64) \n",
    "neighors=range(1,n_neighbors_approx+8)\n",
    "RMSE_score = []\n",
    "    \n",
    "for i in neighors:\n",
    "    \n",
    "        item_similarity_knn=get_item_similarity_knn(train_data,i)\n",
    "        final=join_groundtruth_predicted_value(item_similarity_knn,raw_data,train_data)\n",
    "        rmse=sqrt(mean_squared_error(final.overall_pred, final.overall))\n",
    "                    \n",
    "        RMSE_score.append(rmse)\n",
    "        print ('Item-based CF RMSE for Training Set: ' + str(rmse) + ' for ' + str(i) + ' neighors')\n",
    "    \n",
    "neighors_list=(list(neighors))\n",
    "plt.plot(neighors_list, RMSE_score)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NMF\n",
    "\n",
    "def join_groundtruth_predicted_value_NF(nmf,v_raw_data,v_data,v_i):\n",
    "    \n",
    "    W = nmf.transform(v_data);\n",
    "    H = nmf.components_\n",
    "    item_prediction_NF = np.dot(W,H)\n",
    "\n",
    "    item_prediction_NF=pd.DataFrame(item_prediction_NF, columns=v_data.columns,index=v_data.index)\n",
    "    \n",
    "    item_prediction_NF['reviewerID']=item_prediction_NF.index\n",
    "    prediction_melt=pd.melt(item_prediction_NF, id_vars=['reviewerID'], var_name='asin', value_name='overall_pred')\n",
    "     \n",
    "    final=pd.merge(v_raw_data, prediction_melt, how='inner', on=['reviewerID','asin'])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_features=range(100,500,100)\n",
    "alpha_values=(x * 0.001 for x in range(0, 10,5))\n",
    "l1_ratio_values=(x * 0.01 for x in range(0, 10,5))\n",
    "RMSE_nf_score = []\n",
    "combo=[]\n",
    "import itertools\n",
    "for combination in itertools.product(latent_features, alpha_values, l1_ratio_values):\n",
    "    i=combination[0]\n",
    "    a=combination[1]\n",
    "    l1=combination[2]\n",
    "\n",
    "    nmf = NMF(n_components =i,alpha=a,l1_ratio=l1)\n",
    "    W = nmf.fit(train_data);\n",
    "    final=join_groundtruth_predicted_value_NF(nmf,raw_data,train_data,i)\n",
    "    rmse_nf=sqrt(mean_squared_error(final.overall_pred, final.overall))\n",
    "    combo.append(i)\n",
    "    RMSE_nf_score.append(rmse_nf)\n",
    "    print ('Item-based CF RMSE: ' + str(rmse_nf) + ' for ' + str(i) + ' latent features, '+ str(a) + ' alpha, '+ str(l1) + ' l1_ratio')\n",
    "           \n",
    "\n",
    "latent_features_list=(list(combo))\n",
    "plt.plot(latent_features_list, RMSE_nf_score)\n",
    "plt.xlabel('Number of Latest Features P')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "\n",
    "I initially chose the k neighors based on the sqroot of the total number of samples. I then decided to iterate over 20 values around k to find the minimum RMSE scores. The K respect to the minimum RMSE value is then choses as the final K neighors.\n",
    "\n",
    "In case of NMF, I started with 2 latent features and then iterated over 40 consecutive p values find the minimum RMSE scores. The K respect to the minimum RMSE value is then chosen as the final p latent factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results\n",
    "_(approx. 2-3 pages)_\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "\n",
    "The k neighors and p latent factors are obtained from the training dataset.\n",
    "\n",
    "We then use the optimized parameters and implement the models with test dataset.\n",
    "\n",
    "The RMSE score for KNN in th etest dataset with k neighors is 3\n",
    "The RMSE score for the NMF dataset with p latent factors is 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=32\n",
    "        \n",
    "item_similarity_knn=get_item_similarity_knn(train_data,k)\n",
    "final=join_groundtruth_predicted_value(item_similarity_knn,raw_data,test_data)\n",
    "rmse=sqrt(mean_squared_error(final.overall_pred.astype(int), final.overall.astype(int)))\n",
    "\n",
    "                  \n",
    "print ('Item-based CF RMSE for Test Set using knn: ' + str(rmse) + ' for ' + str(k) + ' neighors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=300\n",
    "nmf = NMF(n_components =p,alpha=0,l1_ratio=0)\n",
    "W = nmf.fit(train_data);\n",
    "final_nf=join_groundtruth_predicted_value_NF(nmf,raw_data,test_data,p)\n",
    "rmse_nf = sqrt(mean_squared_error(final_nf.overall_pred, final_nf.overall))\n",
    "print ('Item-based CF RMSE for Test Set using NF: ' + str(rmse_nf) + ' for ' + str(p) + ' latent features')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_new_data_step1= pd.read_csv(\"item_dedup.csv\", skiprows=7000000,nrows=1000000, header=None)\n",
    "brand_new_data_step1.columns = ['reviewerID', 'asin','overall','reviewTime']\n",
    "brand_new_data_step1.drop(['reviewTime'], axis = 1, inplace = True)\n",
    "\n",
    "brand_new_data=brand_new_data_step1.assign(rnk=brand_new_data_step1.groupby(['overall'])['asin']\n",
    "                    .rank(method='min', ascending=False)) .query('rnk < 100000') \n",
    "brand_new_data.append(final_data)\n",
    "brand_new_data.drop(['rnk'], axis = 1, inplace = True)\n",
    "\n",
    "brand_new_data_pivot=pd.pivot_table(brand_new_data, values = 'overall', index='reviewerID', columns ='asin')\n",
    "brand_new_data_pivot.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "p=300\n",
    "a=0\n",
    "l1=0\n",
    "nmf = NMF(n_components =p, random_state=r,alpha=a,l1_ratio=l1)\n",
    "W = nmf.fit(brand_new_data_pivot);\n",
    "final_nf=join_groundtruth_predicted_value_NF(nmf,brand_new_data,brand_new_data_pivot,p)\n",
    "rmse_nf = sqrt(mean_squared_error(final_nf.overall_pred, final_nf.overall))\n",
    "print ('Item-based CF RMSE for Test Set using NF: ' + str(rmse_nf) + ' for ' + str(p) + ' latent features')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=300\n",
    "r_state=range(43,600,30)\n",
    "rmse_r_state=[]\n",
    "for r in r_state:\n",
    "    nmf = NMF(n_components =p, random_state=r,alpha=a,l1_ratio=l1)\n",
    "    W = nmf.fit(train_data);\n",
    "    final_nf=join_groundtruth_predicted_value_NF(nmf,raw_data,test_data,p)\n",
    "    rmse_nf = sqrt(mean_squared_error(final_nf.overall_pred, final_nf.overall))\n",
    "    rmse_r_state.append(rmse_nf)\n",
    "    print ('Item-based CF RMSE for Test Set using NF: ' + str(rmse_nf) + ' for ' + str(p) + ' neighors')\n",
    "   \n",
    "\n",
    "r_state_list=(list(r_state))\n",
    "plt.plot(r_state_list, rmse_r_state)\n",
    "plt.xlabel('Random State with latent features 300')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()\n",
    "\n",
    "from statistics import mean,variance\n",
    "mean=mean(rmse_r_state)\n",
    "var=variance(rmse_r_state)\n",
    "mean,'{:f}'.format(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=300\n",
    "r_state=range(43,600,30)\n",
    "rmse_r_state=[]\n",
    "for r in r_state:\n",
    "    nmf = NMF(n_components =p, random_state=r)\n",
    "    W = nmf.fit(train_data);\n",
    "    final_nf=join_groundtruth_predicted_value_NF(nmf,raw_data,test_data,p)\n",
    "    rmse_nf = sqrt(mean_squared_error(final_nf.overall_pred, final_nf.overall))\n",
    "    rmse_r_state.append(rmse_nf)\n",
    "    print ('Item-based CF RMSE for Test Set using NF: ' + str(rmse_nf) + ' for ' + str(p) + ' neighors')\n",
    "   \n",
    "\n",
    "r_state_list=(list(r_state))\n",
    "plt.plot(r_state_list, rmse_r_state)\n",
    "plt.xlabel('Random State with latent features 300')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()\n",
    "\n",
    "from statistics import mean,variance\n",
    "mean=mean(rmse_r_state)\n",
    "var=variance(rmse_r_state)\n",
    "mean,'{:f}'.format(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification\n",
    "\n",
    "The results obtained are reasonalbly good buy further improvement can be done.\n",
    "\n",
    "Comparing our results with those in our Benchmark  RMSE score in KNN, we can say that the RMSE score has improved significantly for our NMF Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion\n",
    "_(approx. 1-2 pages)_\n",
    "\n",
    "### Free-Form Visualization\n",
    "\n",
    "The charts below visualizes the prediction accuracy of the KNN and NMF dataset and compares it visually with the original dataset.\n",
    "\n",
    "We also add the predicted rating of the unrated items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(color_codes=True)\n",
    "plt.figure('overall')\n",
    "plt.ylabel('Count of Ratings')\n",
    "plt.xlabel('Ratings 1 to 5')\n",
    "sns.distplot(final['overall'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "plt.figure('overall')\n",
    "plt.ylabel('Count of Ratings')\n",
    "plt.xlabel('Ratings 1 to 5')\n",
    "sns.distplot(final['overall_pred'])\n",
    "plt.show()\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "plt.figure('overall')\n",
    "plt.ylabel('Count of Ratings')\n",
    "plt.xlabel('Ratings 1 to 5')\n",
    "sns.distplot(final_nf['overall_pred'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "### Reflection\n",
    "The Udacity ML Nanodegree Capstone Project has been a challenging yet enriching experience. Each section has been challenging in its way but implementation section has been the most challenging for me. Besides handling 82 milion dataset and reducing it down to a reasonable number that my PC can handle has been a time consuming process as well.\n",
    "\n",
    "\n",
    "### Improvement\n",
    "\n",
    "There are several ways the model can be improved, given more time and more powerful Server:\n",
    "\n",
    "1. Value of k in k neighors and p latent factors in NMF can be optimized by by implementing K fold cross validation. BY splitting the dataset in K folds and then choosing random K-1 folds at a time, K RMSE scores can be generated for a specific k or p values. The k/p value with the lowest average RMSE score can then be marked as optimized and used in the final dataset.\n",
    "\n",
    "2. A few other We can also try a few other matrix factorization algorithm such as SVD and bench mark the performance as well.\n",
    "\n",
    "Since I have have the timestamp for each review, I can use recurrent neural network to provide session based recommendation which will give recommendation on immediate products the user might be interested in."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
